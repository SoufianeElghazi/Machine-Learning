{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55cb33d9",
   "metadata": {},
   "source": [
    "                    ###########################################\n",
    "                    ############___ELGHAZI_SOUFIANE___#########\n",
    "                    ###########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585076ee",
   "metadata": {},
   "source": [
    "## Binary Classification Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3c4fd5",
   "metadata": {},
   "source": [
    " ###            1. Load and Prepare the Dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977ff865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer \n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "##\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba3942f",
   "metadata": {},
   "source": [
    "#####  a. Load the Wisconsin breast cancer dataset into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60260a42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "x, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0864971",
   "metadata": {},
   "source": [
    "####  b.Split the dataset into training and testing sets using the train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ecc2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.170</td>\n",
       "      <td>18.66</td>\n",
       "      <td>85.98</td>\n",
       "      <td>534.6</td>\n",
       "      <td>0.11580</td>\n",
       "      <td>0.12310</td>\n",
       "      <td>0.12260</td>\n",
       "      <td>0.07340</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.06777</td>\n",
       "      <td>...</td>\n",
       "      <td>15.67</td>\n",
       "      <td>27.95</td>\n",
       "      <td>102.80</td>\n",
       "      <td>759.4</td>\n",
       "      <td>0.1786</td>\n",
       "      <td>0.4166</td>\n",
       "      <td>0.50060</td>\n",
       "      <td>0.20880</td>\n",
       "      <td>0.3900</td>\n",
       "      <td>0.11790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.340</td>\n",
       "      <td>12.27</td>\n",
       "      <td>78.94</td>\n",
       "      <td>468.5</td>\n",
       "      <td>0.09003</td>\n",
       "      <td>0.06307</td>\n",
       "      <td>0.02958</td>\n",
       "      <td>0.02647</td>\n",
       "      <td>0.1689</td>\n",
       "      <td>0.05808</td>\n",
       "      <td>...</td>\n",
       "      <td>13.61</td>\n",
       "      <td>19.27</td>\n",
       "      <td>87.22</td>\n",
       "      <td>564.9</td>\n",
       "      <td>0.1292</td>\n",
       "      <td>0.2074</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.10700</td>\n",
       "      <td>0.3110</td>\n",
       "      <td>0.07592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.980</td>\n",
       "      <td>19.62</td>\n",
       "      <td>91.12</td>\n",
       "      <td>599.5</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.11330</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.06463</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>0.06544</td>\n",
       "      <td>...</td>\n",
       "      <td>17.04</td>\n",
       "      <td>30.80</td>\n",
       "      <td>113.90</td>\n",
       "      <td>869.3</td>\n",
       "      <td>0.1613</td>\n",
       "      <td>0.3568</td>\n",
       "      <td>0.40690</td>\n",
       "      <td>0.18270</td>\n",
       "      <td>0.3179</td>\n",
       "      <td>0.10550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.504</td>\n",
       "      <td>12.44</td>\n",
       "      <td>60.34</td>\n",
       "      <td>273.9</td>\n",
       "      <td>0.10240</td>\n",
       "      <td>0.06492</td>\n",
       "      <td>0.02956</td>\n",
       "      <td>0.02076</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>...</td>\n",
       "      <td>10.23</td>\n",
       "      <td>15.66</td>\n",
       "      <td>65.13</td>\n",
       "      <td>314.9</td>\n",
       "      <td>0.1324</td>\n",
       "      <td>0.1148</td>\n",
       "      <td>0.08867</td>\n",
       "      <td>0.06227</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.07773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.880</td>\n",
       "      <td>28.92</td>\n",
       "      <td>82.50</td>\n",
       "      <td>514.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.05824</td>\n",
       "      <td>0.06195</td>\n",
       "      <td>0.02343</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05708</td>\n",
       "      <td>...</td>\n",
       "      <td>13.89</td>\n",
       "      <td>35.74</td>\n",
       "      <td>88.84</td>\n",
       "      <td>595.7</td>\n",
       "      <td>0.1227</td>\n",
       "      <td>0.1620</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.06493</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>0.07242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>13.000</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>15.49</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.53900</td>\n",
       "      <td>0.20600</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>13.800</td>\n",
       "      <td>15.79</td>\n",
       "      <td>90.43</td>\n",
       "      <td>584.1</td>\n",
       "      <td>0.10070</td>\n",
       "      <td>0.12800</td>\n",
       "      <td>0.07789</td>\n",
       "      <td>0.05069</td>\n",
       "      <td>0.1662</td>\n",
       "      <td>0.06566</td>\n",
       "      <td>...</td>\n",
       "      <td>16.57</td>\n",
       "      <td>20.86</td>\n",
       "      <td>110.30</td>\n",
       "      <td>812.4</td>\n",
       "      <td>0.1411</td>\n",
       "      <td>0.3542</td>\n",
       "      <td>0.27790</td>\n",
       "      <td>0.13830</td>\n",
       "      <td>0.2589</td>\n",
       "      <td>0.10300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>17.910</td>\n",
       "      <td>21.02</td>\n",
       "      <td>124.40</td>\n",
       "      <td>994.0</td>\n",
       "      <td>0.12300</td>\n",
       "      <td>0.25760</td>\n",
       "      <td>0.31890</td>\n",
       "      <td>0.11980</td>\n",
       "      <td>0.2113</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>...</td>\n",
       "      <td>20.80</td>\n",
       "      <td>27.78</td>\n",
       "      <td>149.60</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>0.1873</td>\n",
       "      <td>0.5917</td>\n",
       "      <td>0.90340</td>\n",
       "      <td>0.19640</td>\n",
       "      <td>0.3245</td>\n",
       "      <td>0.11980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>15.780</td>\n",
       "      <td>22.91</td>\n",
       "      <td>105.70</td>\n",
       "      <td>782.6</td>\n",
       "      <td>0.11550</td>\n",
       "      <td>0.17520</td>\n",
       "      <td>0.21330</td>\n",
       "      <td>0.09479</td>\n",
       "      <td>0.2096</td>\n",
       "      <td>0.07331</td>\n",
       "      <td>...</td>\n",
       "      <td>20.19</td>\n",
       "      <td>30.50</td>\n",
       "      <td>130.30</td>\n",
       "      <td>1272.0</td>\n",
       "      <td>0.1855</td>\n",
       "      <td>0.4925</td>\n",
       "      <td>0.73560</td>\n",
       "      <td>0.20340</td>\n",
       "      <td>0.3274</td>\n",
       "      <td>0.12520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>9.876</td>\n",
       "      <td>17.27</td>\n",
       "      <td>62.92</td>\n",
       "      <td>295.4</td>\n",
       "      <td>0.10890</td>\n",
       "      <td>0.07232</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.01952</td>\n",
       "      <td>0.1934</td>\n",
       "      <td>0.06285</td>\n",
       "      <td>...</td>\n",
       "      <td>10.42</td>\n",
       "      <td>23.22</td>\n",
       "      <td>67.08</td>\n",
       "      <td>331.6</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>0.06213</td>\n",
       "      <td>0.05588</td>\n",
       "      <td>0.2989</td>\n",
       "      <td>0.07380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0         13.170         18.66           85.98      534.6          0.11580   \n",
       "1         12.340         12.27           78.94      468.5          0.09003   \n",
       "2         13.980         19.62           91.12      599.5          0.10600   \n",
       "3          9.504         12.44           60.34      273.9          0.10240   \n",
       "4         12.880         28.92           82.50      514.3          0.08123   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "393       13.000         21.82           87.50      519.8          0.12730   \n",
       "394       13.800         15.79           90.43      584.1          0.10070   \n",
       "395       17.910         21.02          124.40      994.0          0.12300   \n",
       "396       15.780         22.91          105.70      782.6          0.11550   \n",
       "397        9.876         17.27           62.92      295.4          0.10890   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.12310         0.12260              0.07340         0.2128   \n",
       "1             0.06307         0.02958              0.02647         0.1689   \n",
       "2             0.11330         0.11260              0.06463         0.1669   \n",
       "3             0.06492         0.02956              0.02076         0.1815   \n",
       "4             0.05824         0.06195              0.02343         0.1566   \n",
       "..                ...             ...                  ...            ...   \n",
       "393           0.19320         0.18590              0.09353         0.2350   \n",
       "394           0.12800         0.07789              0.05069         0.1662   \n",
       "395           0.25760         0.31890              0.11980         0.2113   \n",
       "396           0.17520         0.21330              0.09479         0.2096   \n",
       "397           0.07232         0.01756              0.01952         0.1934   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.06777  ...         15.67          27.95   \n",
       "1                   0.05808  ...         13.61          19.27   \n",
       "2                   0.06544  ...         17.04          30.80   \n",
       "3                   0.06905  ...         10.23          15.66   \n",
       "4                   0.05708  ...         13.89          35.74   \n",
       "..                      ...  ...           ...            ...   \n",
       "393                 0.07389  ...         15.49          30.73   \n",
       "394                 0.06566  ...         16.57          20.86   \n",
       "395                 0.07115  ...         20.80          27.78   \n",
       "396                 0.07331  ...         20.19          30.50   \n",
       "397                 0.06285  ...         10.42          23.22   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0             102.80       759.4            0.1786             0.4166   \n",
       "1              87.22       564.9            0.1292             0.2074   \n",
       "2             113.90       869.3            0.1613             0.3568   \n",
       "3              65.13       314.9            0.1324             0.1148   \n",
       "4              88.84       595.7            0.1227             0.1620   \n",
       "..               ...         ...               ...                ...   \n",
       "393           106.20       739.3            0.1703             0.5401   \n",
       "394           110.30       812.4            0.1411             0.3542   \n",
       "395           149.60      1304.0            0.1873             0.5917   \n",
       "396           130.30      1272.0            0.1855             0.4925   \n",
       "397            67.08       331.6            0.1415             0.1247   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0            0.50060               0.20880          0.3900   \n",
       "1            0.17910               0.10700          0.3110   \n",
       "2            0.40690               0.18270          0.3179   \n",
       "3            0.08867               0.06227          0.2450   \n",
       "4            0.24390               0.06493          0.2372   \n",
       "..               ...                   ...             ...   \n",
       "393          0.53900               0.20600          0.4378   \n",
       "394          0.27790               0.13830          0.2589   \n",
       "395          0.90340               0.19640          0.3245   \n",
       "396          0.73560               0.20340          0.3274   \n",
       "397          0.06213               0.05588          0.2989   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11790  \n",
       "1                    0.07592  \n",
       "2                    0.10550  \n",
       "3                    0.07773  \n",
       "4                    0.07242  \n",
       "..                       ...  \n",
       "393                  0.10720  \n",
       "394                  0.10300  \n",
       "395                  0.11980  \n",
       "396                  0.12520  \n",
       "397                  0.07380  \n",
       "\n",
       "[398 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=5)\n",
    "\n",
    "df = pd.DataFrame(x_train, columns=data.feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48792802",
   "metadata": {},
   "source": [
    "#### c. Print the number of instances and features in the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "420a090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --Number of instances and features in the training set:\n",
      "------------------------------------------------------\n",
      "Training instances: 398\n",
      "Training features: 30\n",
      "______________________________________________________\n",
      "\n",
      "  --Number of instances and features in the testing set:\n",
      "---------------------------------------------------------\n",
      "Testing instances: 171\n",
      "Testing features: 30\n"
     ]
    }
   ],
   "source": [
    "print(\"  --Number of instances and features in the training set:\")\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Training instances:\", x_train.shape[0])\n",
    "print(\"Training features:\", x_train.shape[1])\n",
    "print(\"______________________________________________________\")\n",
    "print(\"\\n  --Number of instances and features in the testing set:\")\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"Testing instances:\", x_test.shape[0])\n",
    "print(\"Testing features:\", x_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d3ef27",
   "metadata": {},
   "source": [
    "###            2. Train and Test Multiple Classifiers: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121b4e3e",
   "metadata": {},
   "source": [
    "#### a. Train a KNN classifier using the KNeighborsClassifier() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a840a1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model_1 = KNeighborsClassifier(n_neighbors=5)\n",
    "model_1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c769eb5",
   "metadata": {},
   "source": [
    "#### b. Train a Decision Tree classifier using the DecisionTreeClassifier() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8650b253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=42)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model_2 = DecisionTreeClassifier(random_state=42)\n",
    "model_2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210c6212",
   "metadata": {},
   "source": [
    "#### c. Train a Logistic Regression classifier using the LogisticRegression() function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0822885b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_3 = LogisticRegression(random_state=0)\n",
    "model_3.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab5bb1",
   "metadata": {},
   "source": [
    "####  d. Train a Random Forest classifier using the RandomForestClassifier() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99990f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_4 = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "model_4.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b42286f",
   "metadata": {},
   "source": [
    "#### e. Train a Support Vector Machine (SVM) classifier using the SVC() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "640f7226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(random_state=0, tol=1e-05)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model_5 = LinearSVC(random_state=0, tol=1e-5)\n",
    "model_5.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111b421a",
   "metadata": {},
   "source": [
    "#### f. Evaluate the performance of each classifier on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b61ce1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ___KNeighborsClassifier___\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.96      0.89      0.92        61\n",
      "     class 2       0.94      0.98      0.96       110\n",
      "\n",
      "    accuracy                           0.95       171\n",
      "   macro avg       0.95      0.93      0.94       171\n",
      "weighted avg       0.95      0.95      0.95       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['class 1', 'class 2']\n",
    "\n",
    "                        ###########################################\n",
    "                        ############___KNeighborsClassifier___#####\n",
    "                        ###########################################\n",
    "y_predec_model_1 =model_1.predict(x_test)\n",
    "print(\"                     ___KNeighborsClassifier___\")            \n",
    "print(classification_report(y_test, y_predec_model_1, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f75f5882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ___DecisionTreeClassifier___\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.92      0.92      0.92        61\n",
      "     class 2       0.95      0.95      0.95       110\n",
      "\n",
      "    accuracy                           0.94       171\n",
      "   macro avg       0.94      0.94      0.94       171\n",
      "weighted avg       0.94      0.94      0.94       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                        ###########################################\n",
    "                        ############___DecisionTreeClassifier___###\n",
    "                        ###########################################\n",
    "y_predec_model_2 =model_2.predict(x_test)\n",
    "print(\"                     ___DecisionTreeClassifier___\")            \n",
    "print(classification_report(y_test, y_predec_model_2, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0348a414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ___LogisticRegression___\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.98      0.95      0.97        61\n",
      "     class 2       0.97      0.99      0.98       110\n",
      "\n",
      "    accuracy                           0.98       171\n",
      "   macro avg       0.98      0.97      0.97       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                        ###########################################\n",
    "                        ############___LogisticRegression___#######\n",
    "                        ###########################################\n",
    "y_predec_model_3 =model_3.predict(x_test)\n",
    "print(\"                     ___LogisticRegression___\")            \n",
    "print(classification_report(y_test, y_predec_model_3, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f19c4c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ___RandomForestClassifier___\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.97      0.95      0.96        61\n",
      "     class 2       0.97      0.98      0.98       110\n",
      "\n",
      "    accuracy                           0.97       171\n",
      "   macro avg       0.97      0.97      0.97       171\n",
      "weighted avg       0.97      0.97      0.97       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                        ###########################################\n",
    "                        ############___RandomForestClassifier___###\n",
    "                        ###########################################\n",
    "y_predec_model_4 =model_4.predict(x_test)\n",
    "print(\"                     ___RandomForestClassifier___\")            \n",
    "print(classification_report(y_test, y_predec_model_4, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30984370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ___Support Vector Machine (SVM)___\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.97      0.95      0.96        61\n",
      "     class 2       0.97      0.98      0.98       110\n",
      "\n",
      "    accuracy                           0.97       171\n",
      "   macro avg       0.97      0.97      0.97       171\n",
      "weighted avg       0.97      0.97      0.97       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                        ###########################################\n",
    "                        ####___Support Vector Machine (SVM) ___####\n",
    "                        ###########################################\n",
    "y_predec_model_5 =model_5.predict(x_test)\n",
    "print(\"               ___Support Vector Machine (SVM)___\")            \n",
    "print(classification_report(y_test, y_predec_model_4, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68624d",
   "metadata": {},
   "source": [
    "###                    3. Tune Hyperparameters: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9029a07e",
   "metadata": {},
   "source": [
    "##### a. Use GridSearchCV to find the best hyperparameters for the KNN classifier.Vary the number of neighbors from 1 to 10 and use 5-fold cross-validation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11efc4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ___KNeighborsClassifier___\n",
      "______________________________________________________\n",
      "Best hyperparameters:  {'n_neighbors': 10}\n",
      "Best accuracy score:  0.9220569620253165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'n_neighbors': range(1, 11)}\n",
    "grid_search = GridSearchCV(model_1, param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "print(\"                     ___KNeighborsClassifier___\") \n",
    "print(\"______________________________________________________\") \n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd3596",
   "metadata": {},
   "source": [
    "#### b. Use GridSearchCV to find the best hyperparameters for the Decision Tree classifier. Vary the maximum depth from 1 to 10 and use 5-fold cross_validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a9804ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ___DecisionTreeClassifier___\n",
      "______________________________________________________\n",
      "Best hyperparameters:  {'max_depth': 3}\n",
      "Best accuracy score:  0.9295886075949367\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_depth': range(1, 11)}\n",
    "grid_search = GridSearchCV(model_2, param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "print(\"                     ___DecisionTreeClassifier___\") \n",
    "print(\"______________________________________________________\")\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b802f4",
   "metadata": {},
   "source": [
    "#### c. Use GridSearchCV to find the best hyperparameters for the Logistic Regression classifier. Vary the regularization parameter C from 0.01 to 10 and use 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "670a866e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ___LogisticRegression___\n",
      "______________________________________________________\n",
      "Best hyperparameters:  {'C': 1}\n",
      "Best accuracy score:  0.9421518987341774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1, 10]}\n",
    "grid_search = GridSearchCV(model_3, param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "print(\"                     ___LogisticRegression___\")\n",
    "print(\"______________________________________________________\")\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b5a57d",
   "metadata": {},
   "source": [
    "#### d. Use GridSearchCV to find the best hyperparameters for the Random Forest classifier. Vary the number of trees from 50 to 200 and use 5-fold cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b20406fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ___RandomForestClassifier___\n",
      "______________________________________________________\n",
      "Best parameters:  {'n_estimators': 55}\n",
      "Best accuracy score:  0.9623101265822787\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_estimators': range(50, 201)}\n",
    "grid_search = GridSearchCV(model_4, param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "print(\"                     ___RandomForestClassifier___\")  \n",
    "print(\"______________________________________________________\")\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5fa45",
   "metadata": {},
   "source": [
    "#### e. Use GridSearchCV to find the best hyperparameters for the SVM classifier. Vary the regularization parameter C from 0.01 to 10 and use 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6572264f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ___Support Vector Machine (SVM)___\n",
      "______________________________________________________\n",
      "Best parameters:  {'C': 10}\n",
      "Best accuracy score:  0.901867088607595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1, 10]}\n",
    "grid_search = GridSearchCV(model_5, param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "print(\"               ___Support Vector Machine (SVM)___\") \n",
    "print(\"______________________________________________________\")\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe1c65e",
   "metadata": {},
   "source": [
    "#### f. Print the best hyperparameters and the corresponding performance metrics for each classifier."
   ]
  },
  {
   "cell_type": "raw",
   "id": "876c0b03",
   "metadata": {},
   "source": [
    "####################################__Included in the previous cells!__######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28865866",
   "metadata": {},
   "source": [
    "### 4. Compare and Analyze Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9e38ae",
   "metadata": {},
   "source": [
    "#### a. Compare the performance of each classifier before and after hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a6fa99",
   "metadata": {},
   "source": [
    "    En se basent sur les résultats affichés, on trouve que le max de l’accuracy est 98% par le classificateur LogisticRegression, suivi par RandomForestClassifier et SVM de précision 97%, puis KNeighborsClassifier 95% et enfin DecisionTreeClassifier 94%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e423f8",
   "metadata": {},
   "source": [
    "#### b. Analyze the best hyperparameters for each classifier and their impact on the performance metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071aafd5",
   "metadata": {},
   "source": [
    "    Après Tuning, on remarque que l’hyperparamètres ont affecté la précision score telle que RandomForestClassifier qui a la valeur maximale 96,23%, suivi par LogisticRegression 94,21%, puis KNeighborsClassifier et DecisionTreeClassifier par 92% et enfin SVM 90,18%.\n",
    "    Alors on remarque que l’ordre a changé ainsi que la précision à diminuer !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d91d410",
   "metadata": {},
   "source": [
    "#### c. Discuss the strengths and weaknesses of each classifier and their suitability for this particular dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1977713a",
   "metadata": {},
   "source": [
    "###### \tKNeighborsClassifier:\n",
    "Le KNN utilise la classification de voisinage comme valeur de prédication de la nouvelle requête. Il a des avantages - l'architecture non paramétrique, simple et puissante, ne nécessite aucun temps d'apprentissage, mais il a aussi un inconvénient - gourmand en mémoire, la classification et l'estimation sont lentes.\n",
    "###### \tDecisionTreeClassifier :\n",
    "Avantages des arbres de décision : Interprétabilité. Moins de préparation des données. Non paramétrique. Polyvalence. Non-linéarité.\n",
    "Inconvénients de l'arbre de décision : Surajustement. Réduction des fonctionnalités et rééchantillonnage des données. Optimisation.\n",
    "###### \tLogisticRegression\n",
    "Avantages : Algorithme d'apprentissage automatique le plus simple et Moins sujet au surajustement et Plus précise\n",
    "Inconvénients : Surajustement, Problème avec les relations complexes \n",
    "###### \tRandomForestClassifier\n",
    "Avantages : Parmi toutes les méthodes de classification disponibles, les forêts aléatoires offrent la plus grande précision. La technique de la forêt aléatoire peut également gérer des données volumineuses avec de nombreuses variables se comptant en milliers. Il peut automatiquement équilibrer les ensembles de données lorsqu'une classe est moins fréquente que d'autres classes dans les données\n",
    "Inconvénients : La principale limitation de la forêt aléatoire est qu'un grand nombre d'arbres peut rendre l'algorithme trop lent et inefficace pour les prédictions en temps réel. En général, ces algorithmes sont rapides à former, mais assez lents à créer des prédictions une fois qu'ils sont formés.\n",
    "###### \tSVC\n",
    "Avantages : SVM fonctionne relativement bien lorsqu'il existe une marge de séparation claire entre les classes. SVM est plus efficace dans les espaces de grande dimension et est relativement économe en mémoire. SVM est efficace dans les cas où les dimensions sont supérieures au nombre d'échantillons.\n",
    "Inconvénients : Il ne fonctionne pas bien lorsque nous avons un grand ensemble de données car le temps de formation requis est plus élevé.\n",
    "Il ne fonctionne pas non plus très bien lorsque l'ensemble de données a plus de bruit, c'est-à-dire que les classes cibles se chevauchent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e5408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
