{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55cb33d9",
   "metadata": {},
   "source": [
    "                    ###########################################\n",
    "                    ############___ELGHAZI_SOUFIANE___#########\n",
    "                    ###########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585076ee",
   "metadata": {},
   "source": [
    "## Binary Classification Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3c4fd5",
   "metadata": {},
   "source": [
    " ###            1. Load and Prepare the Dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977ff865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer \n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "##\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba3942f",
   "metadata": {},
   "source": [
    "#####  a. Load the Wisconsin breast cancer dataset into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60260a42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "x, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0864971",
   "metadata": {},
   "source": [
    "####  b.Split the dataset into training and testing sets using the train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ecc2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.170</td>\n",
       "      <td>18.66</td>\n",
       "      <td>85.98</td>\n",
       "      <td>534.6</td>\n",
       "      <td>0.11580</td>\n",
       "      <td>0.12310</td>\n",
       "      <td>0.12260</td>\n",
       "      <td>0.07340</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.06777</td>\n",
       "      <td>...</td>\n",
       "      <td>15.67</td>\n",
       "      <td>27.95</td>\n",
       "      <td>102.80</td>\n",
       "      <td>759.4</td>\n",
       "      <td>0.1786</td>\n",
       "      <td>0.4166</td>\n",
       "      <td>0.50060</td>\n",
       "      <td>0.20880</td>\n",
       "      <td>0.3900</td>\n",
       "      <td>0.11790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.340</td>\n",
       "      <td>12.27</td>\n",
       "      <td>78.94</td>\n",
       "      <td>468.5</td>\n",
       "      <td>0.09003</td>\n",
       "      <td>0.06307</td>\n",
       "      <td>0.02958</td>\n",
       "      <td>0.02647</td>\n",
       "      <td>0.1689</td>\n",
       "      <td>0.05808</td>\n",
       "      <td>...</td>\n",
       "      <td>13.61</td>\n",
       "      <td>19.27</td>\n",
       "      <td>87.22</td>\n",
       "      <td>564.9</td>\n",
       "      <td>0.1292</td>\n",
       "      <td>0.2074</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.10700</td>\n",
       "      <td>0.3110</td>\n",
       "      <td>0.07592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.980</td>\n",
       "      <td>19.62</td>\n",
       "      <td>91.12</td>\n",
       "      <td>599.5</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.11330</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.06463</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>0.06544</td>\n",
       "      <td>...</td>\n",
       "      <td>17.04</td>\n",
       "      <td>30.80</td>\n",
       "      <td>113.90</td>\n",
       "      <td>869.3</td>\n",
       "      <td>0.1613</td>\n",
       "      <td>0.3568</td>\n",
       "      <td>0.40690</td>\n",
       "      <td>0.18270</td>\n",
       "      <td>0.3179</td>\n",
       "      <td>0.10550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.504</td>\n",
       "      <td>12.44</td>\n",
       "      <td>60.34</td>\n",
       "      <td>273.9</td>\n",
       "      <td>0.10240</td>\n",
       "      <td>0.06492</td>\n",
       "      <td>0.02956</td>\n",
       "      <td>0.02076</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>...</td>\n",
       "      <td>10.23</td>\n",
       "      <td>15.66</td>\n",
       "      <td>65.13</td>\n",
       "      <td>314.9</td>\n",
       "      <td>0.1324</td>\n",
       "      <td>0.1148</td>\n",
       "      <td>0.08867</td>\n",
       "      <td>0.06227</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.07773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.880</td>\n",
       "      <td>28.92</td>\n",
       "      <td>82.50</td>\n",
       "      <td>514.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.05824</td>\n",
       "      <td>0.06195</td>\n",
       "      <td>0.02343</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05708</td>\n",
       "      <td>...</td>\n",
       "      <td>13.89</td>\n",
       "      <td>35.74</td>\n",
       "      <td>88.84</td>\n",
       "      <td>595.7</td>\n",
       "      <td>0.1227</td>\n",
       "      <td>0.1620</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.06493</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>0.07242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>13.000</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>15.49</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.53900</td>\n",
       "      <td>0.20600</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>13.800</td>\n",
       "      <td>15.79</td>\n",
       "      <td>90.43</td>\n",
       "      <td>584.1</td>\n",
       "      <td>0.10070</td>\n",
       "      <td>0.12800</td>\n",
       "      <td>0.07789</td>\n",
       "      <td>0.05069</td>\n",
       "      <td>0.1662</td>\n",
       "      <td>0.06566</td>\n",
       "      <td>...</td>\n",
       "      <td>16.57</td>\n",
       "      <td>20.86</td>\n",
       "      <td>110.30</td>\n",
       "      <td>812.4</td>\n",
       "      <td>0.1411</td>\n",
       "      <td>0.3542</td>\n",
       "      <td>0.27790</td>\n",
       "      <td>0.13830</td>\n",
       "      <td>0.2589</td>\n",
       "      <td>0.10300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>17.910</td>\n",
       "      <td>21.02</td>\n",
       "      <td>124.40</td>\n",
       "      <td>994.0</td>\n",
       "      <td>0.12300</td>\n",
       "      <td>0.25760</td>\n",
       "      <td>0.31890</td>\n",
       "      <td>0.11980</td>\n",
       "      <td>0.2113</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>...</td>\n",
       "      <td>20.80</td>\n",
       "      <td>27.78</td>\n",
       "      <td>149.60</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>0.1873</td>\n",
       "      <td>0.5917</td>\n",
       "      <td>0.90340</td>\n",
       "      <td>0.19640</td>\n",
       "      <td>0.3245</td>\n",
       "      <td>0.11980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>15.780</td>\n",
       "      <td>22.91</td>\n",
       "      <td>105.70</td>\n",
       "      <td>782.6</td>\n",
       "      <td>0.11550</td>\n",
       "      <td>0.17520</td>\n",
       "      <td>0.21330</td>\n",
       "      <td>0.09479</td>\n",
       "      <td>0.2096</td>\n",
       "      <td>0.07331</td>\n",
       "      <td>...</td>\n",
       "      <td>20.19</td>\n",
       "      <td>30.50</td>\n",
       "      <td>130.30</td>\n",
       "      <td>1272.0</td>\n",
       "      <td>0.1855</td>\n",
       "      <td>0.4925</td>\n",
       "      <td>0.73560</td>\n",
       "      <td>0.20340</td>\n",
       "      <td>0.3274</td>\n",
       "      <td>0.12520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>9.876</td>\n",
       "      <td>17.27</td>\n",
       "      <td>62.92</td>\n",
       "      <td>295.4</td>\n",
       "      <td>0.10890</td>\n",
       "      <td>0.07232</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.01952</td>\n",
       "      <td>0.1934</td>\n",
       "      <td>0.06285</td>\n",
       "      <td>...</td>\n",
       "      <td>10.42</td>\n",
       "      <td>23.22</td>\n",
       "      <td>67.08</td>\n",
       "      <td>331.6</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>0.06213</td>\n",
       "      <td>0.05588</td>\n",
       "      <td>0.2989</td>\n",
       "      <td>0.07380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0         13.170         18.66           85.98      534.6          0.11580   \n",
       "1         12.340         12.27           78.94      468.5          0.09003   \n",
       "2         13.980         19.62           91.12      599.5          0.10600   \n",
       "3          9.504         12.44           60.34      273.9          0.10240   \n",
       "4         12.880         28.92           82.50      514.3          0.08123   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "393       13.000         21.82           87.50      519.8          0.12730   \n",
       "394       13.800         15.79           90.43      584.1          0.10070   \n",
       "395       17.910         21.02          124.40      994.0          0.12300   \n",
       "396       15.780         22.91          105.70      782.6          0.11550   \n",
       "397        9.876         17.27           62.92      295.4          0.10890   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.12310         0.12260              0.07340         0.2128   \n",
       "1             0.06307         0.02958              0.02647         0.1689   \n",
       "2             0.11330         0.11260              0.06463         0.1669   \n",
       "3             0.06492         0.02956              0.02076         0.1815   \n",
       "4             0.05824         0.06195              0.02343         0.1566   \n",
       "..                ...             ...                  ...            ...   \n",
       "393           0.19320         0.18590              0.09353         0.2350   \n",
       "394           0.12800         0.07789              0.05069         0.1662   \n",
       "395           0.25760         0.31890              0.11980         0.2113   \n",
       "396           0.17520         0.21330              0.09479         0.2096   \n",
       "397           0.07232         0.01756              0.01952         0.1934   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.06777  ...         15.67          27.95   \n",
       "1                   0.05808  ...         13.61          19.27   \n",
       "2                   0.06544  ...         17.04          30.80   \n",
       "3                   0.06905  ...         10.23          15.66   \n",
       "4                   0.05708  ...         13.89          35.74   \n",
       "..                      ...  ...           ...            ...   \n",
       "393                 0.07389  ...         15.49          30.73   \n",
       "394                 0.06566  ...         16.57          20.86   \n",
       "395                 0.07115  ...         20.80          27.78   \n",
       "396                 0.07331  ...         20.19          30.50   \n",
       "397                 0.06285  ...         10.42          23.22   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0             102.80       759.4            0.1786             0.4166   \n",
       "1              87.22       564.9            0.1292             0.2074   \n",
       "2             113.90       869.3            0.1613             0.3568   \n",
       "3              65.13       314.9            0.1324             0.1148   \n",
       "4              88.84       595.7            0.1227             0.1620   \n",
       "..               ...         ...               ...                ...   \n",
       "393           106.20       739.3            0.1703             0.5401   \n",
       "394           110.30       812.4            0.1411             0.3542   \n",
       "395           149.60      1304.0            0.1873             0.5917   \n",
       "396           130.30      1272.0            0.1855             0.4925   \n",
       "397            67.08       331.6            0.1415             0.1247   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0            0.50060               0.20880          0.3900   \n",
       "1            0.17910               0.10700          0.3110   \n",
       "2            0.40690               0.18270          0.3179   \n",
       "3            0.08867               0.06227          0.2450   \n",
       "4            0.24390               0.06493          0.2372   \n",
       "..               ...                   ...             ...   \n",
       "393          0.53900               0.20600          0.4378   \n",
       "394          0.27790               0.13830          0.2589   \n",
       "395          0.90340               0.19640          0.3245   \n",
       "396          0.73560               0.20340          0.3274   \n",
       "397          0.06213               0.05588          0.2989   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11790  \n",
       "1                    0.07592  \n",
       "2                    0.10550  \n",
       "3                    0.07773  \n",
       "4                    0.07242  \n",
       "..                       ...  \n",
       "393                  0.10720  \n",
       "394                  0.10300  \n",
       "395                  0.11980  \n",
       "396                  0.12520  \n",
       "397                  0.07380  \n",
       "\n",
       "[398 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=5)\n",
    "\n",
    "df = pd.DataFrame(x_train, columns=data.feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48792802",
   "metadata": {},
   "source": [
    "#### c. Print the number of instances and features in the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "420a090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  --Number of instances and features in the training set:\n",
      "------------------------------------------------------\n",
      "Training instances: 398\n",
      "Training features: 30\n",
      "______________________________________________________\n",
      "\n",
      "  --Number of instances and features in the testing set:\n",
      "---------------------------------------------------------\n",
      "Testing instances: 171\n",
      "Testing features: 30\n"
     ]
    }
   ],
   "source": [
    "print(\"  --Number of instances and features in the training set:\")\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Training instances:\", x_train.shape[0])\n",
    "print(\"Training features:\", x_train.shape[1])\n",
    "print(\"______________________________________________________\")\n",
    "print(\"\\n  --Number of instances and features in the testing set:\")\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"Testing instances:\", x_test.shape[0])\n",
    "print(\"Testing features:\", x_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d3ef27",
   "metadata": {},
   "source": [
    "###            2. Train and Test Multiple Classifiers: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121b4e3e",
   "metadata": {},
   "source": [
    "#### a. Train a KNN classifier using the KNeighborsClassifier() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a840a1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model_1 = KNeighborsClassifier(n_neighbors=5)\n",
    "model_1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c769eb5",
   "metadata": {},
   "source": [
    "#### b. Train a Decision Tree classifier using the DecisionTreeClassifier() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8650b253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=42)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model_2 = DecisionTreeClassifier(random_state=42)\n",
    "model_2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210c6212",
   "metadata": {},
   "source": [
    "#### c. Train a Logistic Regression classifier using the LogisticRegression() function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0822885b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_3 = LogisticRegression(random_state=0)\n",
    "model_3.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab5bb1",
   "metadata": {},
   "source": [
    "####  d. Train a Random Forest classifier using the RandomForestClassifier() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99990f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_4 = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "model_4.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b42286f",
   "metadata": {},
   "source": [
    "#### e. Train a Support Vector Machine (SVM) classifier using the SVC() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "640f7226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(random_state=0, tol=1e-05)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model_5 = LinearSVC(random_state=0, tol=1e-5)\n",
    "model_5.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111b421a",
   "metadata": {},
   "source": [
    "#### f. Evaluate the performance of each classifier on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b61ce1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ___KNeighborsClassifier___\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.96      0.89      0.92        61\n",
      "     class 2       0.94      0.98      0.96       110\n",
      "\n",
      "    accuracy                           0.95       171\n",
      "   macro avg       0.95      0.93      0.94       171\n",
      "weighted avg       0.95      0.95      0.95       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['class 1', 'class 2']\n",
    "\n",
    "                        ###########################################\n",
    "                        ############___KNeighborsClassifier___#####\n",
    "                        ###########################################\n",
    "y_predec_model_1 =model_1.predict(x_test)\n",
    "print(\"                     ___KNeighborsClassifier___\")            \n",
    "print(classification_report(y_test, y_predec_model_1, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f75f5882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ___DecisionTreeClassifier___\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.92      0.92      0.92        61\n",
      "     class 2       0.95      0.95      0.95       110\n",
      "\n",
      "    accuracy                           0.94       171\n",
      "   macro avg       0.94      0.94      0.94       171\n",
      "weighted avg       0.94      0.94      0.94       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                        ###########################################\n",
    "                        ############___DecisionTreeClassifier___###\n",
    "                        ###########################################\n",
    "y_predec_model_2 =model_2.predict(x_test)\n",
    "print(\"                     ___DecisionTreeClassifier___\")            \n",
    "print(classification_report(y_test, y_predec_model_2, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0348a414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ___LogisticRegression___\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.98      0.95      0.97        61\n",
      "     class 2       0.97      0.99      0.98       110\n",
      "\n",
      "    accuracy                           0.98       171\n",
      "   macro avg       0.98      0.97      0.97       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                        ###########################################\n",
    "                        ############___LogisticRegression___#######\n",
    "                        ###########################################\n",
    "y_predec_model_3 =model_3.predict(x_test)\n",
    "print(\"                     ___LogisticRegression___\")            \n",
    "print(classification_report(y_test, y_predec_model_3, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f19c4c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ___RandomForestClassifier___\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.97      0.95      0.96        61\n",
      "     class 2       0.97      0.98      0.98       110\n",
      "\n",
      "    accuracy                           0.97       171\n",
      "   macro avg       0.97      0.97      0.97       171\n",
      "weighted avg       0.97      0.97      0.97       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                        ###########################################\n",
    "                        ############___RandomForestClassifier___###\n",
    "                        ###########################################\n",
    "y_predec_model_4 =model_4.predict(x_test)\n",
    "print(\"                     ___RandomForestClassifier___\")            \n",
    "print(classification_report(y_test, y_predec_model_4, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30984370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ___Support Vector Machine (SVM)___\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.97      0.95      0.96        61\n",
      "     class 2       0.97      0.98      0.98       110\n",
      "\n",
      "    accuracy                           0.97       171\n",
      "   macro avg       0.97      0.97      0.97       171\n",
      "weighted avg       0.97      0.97      0.97       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                        ###########################################\n",
    "                        ####___Support Vector Machine (SVM) ___####\n",
    "                        ###########################################\n",
    "y_predec_model_5 =model_5.predict(x_test)\n",
    "print(\"               ___Support Vector Machine (SVM)___\")            \n",
    "print(classification_report(y_test, y_predec_model_4, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68624d",
   "metadata": {},
   "source": [
    "###                    3. Tune Hyperparameters: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9029a07e",
   "metadata": {},
   "source": [
    "##### a. Use GridSearchCV to find the best hyperparameters for the KNN classifier.Vary the number of neighbors from 1 to 10 and use 5-fold cross-validation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11efc4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ___KNeighborsClassifier___\n",
      "______________________________________________________\n",
      "Best hyperparameters:  {'n_neighbors': 10}\n",
      "Best accuracy score:  0.9220569620253165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'n_neighbors': range(1, 11)}\n",
    "grid_search = GridSearchCV(model_1, param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "print(\"                     ___KNeighborsClassifier___\") \n",
    "print(\"______________________________________________________\") \n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd3596",
   "metadata": {},
   "source": [
    "#### b. Use GridSearchCV to find the best hyperparameters for the Decision Tree classifier. Vary the maximum depth from 1 to 10 and use 5-fold cross_validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a9804ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ___DecisionTreeClassifier___\n",
      "______________________________________________________\n",
      "Best hyperparameters:  {'max_depth': 3}\n",
      "Best accuracy score:  0.9295886075949367\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_depth': range(1, 11)}\n",
    "grid_search = GridSearchCV(model_2, param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "print(\"                     ___DecisionTreeClassifier___\") \n",
    "print(\"______________________________________________________\")\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b802f4",
   "metadata": {},
   "source": [
    "#### c. Use GridSearchCV to find the best hyperparameters for the Logistic Regression classifier. Vary the regularization parameter C from 0.01 to 10 and use 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "670a866e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ___LogisticRegression___\n",
      "______________________________________________________\n",
      "Best hyperparameters:  {'C': 1}\n",
      "Best accuracy score:  0.9421518987341774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1, 10]}\n",
    "grid_search = GridSearchCV(model_3, param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "print(\"                     ___LogisticRegression___\")\n",
    "print(\"______________________________________________________\")\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b5a57d",
   "metadata": {},
   "source": [
    "#### d. Use GridSearchCV to find the best hyperparameters for the Random Forest classifier. Vary the number of trees from 50 to 200 and use 5-fold cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b20406fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ___RandomForestClassifier___\n",
      "______________________________________________________\n",
      "Best parameters:  {'n_estimators': 55}\n",
      "Best accuracy score:  0.9623101265822787\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_estimators': range(50, 201)}\n",
    "grid_search = GridSearchCV(model_4, param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "print(\"                     ___RandomForestClassifier___\")  \n",
    "print(\"______________________________________________________\")\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5fa45",
   "metadata": {},
   "source": [
    "#### e. Use GridSearchCV to find the best hyperparameters for the SVM classifier. Vary the regularization parameter C from 0.01 to 10 and use 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6572264f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ___Support Vector Machine (SVM)___\n",
      "______________________________________________________\n",
      "Best parameters:  {'C': 10}\n",
      "Best accuracy score:  0.901867088607595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elgha\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1, 10]}\n",
    "grid_search = GridSearchCV(model_5, param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "print(\"               ___Support Vector Machine (SVM)___\") \n",
    "print(\"______________________________________________________\")\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe1c65e",
   "metadata": {},
   "source": [
    "#### f. Print the best hyperparameters and the corresponding performance metrics for each classifier."
   ]
  },
  {
   "cell_type": "raw",
   "id": "876c0b03",
   "metadata": {},
   "source": [
    "####################################__Included in the previous cells!__######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28865866",
   "metadata": {},
   "source": [
    "### 4. Compare and Analyze Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9e38ae",
   "metadata": {},
   "source": [
    "#### a. Compare the performance of each classifier before and after hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a6fa99",
   "metadata": {},
   "source": [
    "    En se basent sur les rÃ©sultats affichÃ©s, on trouve que le max de lâ€™accuracy est 98% par le classificateur LogisticRegression, suivi par RandomForestClassifier et SVM de prÃ©cision 97%, puis KNeighborsClassifier 95% et enfin DecisionTreeClassifier 94%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e423f8",
   "metadata": {},
   "source": [
    "#### b. Analyze the best hyperparameters for each classifier and their impact on the performance metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071aafd5",
   "metadata": {},
   "source": [
    "    AprÃ¨s Tuning, on remarque que lâ€™hyperparamÃ¨tres ont affectÃ© la prÃ©cision score telle que RandomForestClassifier qui a la valeur maximale 96,23%, suivi par LogisticRegression 94,21%, puis KNeighborsClassifier et DecisionTreeClassifier par 92% et enfin SVM 90,18%.\n",
    "    Alors on remarque que lâ€™ordre a changÃ© ainsi que la prÃ©cision Ã  diminuer !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d91d410",
   "metadata": {},
   "source": [
    "#### c. Discuss the strengths and weaknesses of each classifier and their suitability for this particular dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1977713a",
   "metadata": {},
   "source": [
    "###### \tKNeighborsClassifier:\n",
    "Le KNN utilise la classification de voisinage comme valeur de prÃ©dication de la nouvelle requÃªte. Il a des avantages - l'architecture non paramÃ©trique, simple et puissante, ne nÃ©cessite aucun temps d'apprentissage, mais il a aussi un inconvÃ©nient - gourmand en mÃ©moire, la classification et l'estimation sont lentes.\n",
    "###### \tDecisionTreeClassifier :\n",
    "Avantages des arbres de dÃ©cision : InterprÃ©tabilitÃ©. Moins de prÃ©paration des donnÃ©es. Non paramÃ©trique. Polyvalence. Non-linÃ©aritÃ©.\n",
    "InconvÃ©nients de l'arbre de dÃ©cision : Surajustement. RÃ©duction des fonctionnalitÃ©s et rÃ©Ã©chantillonnage des donnÃ©es. Optimisation.\n",
    "###### \tLogisticRegression\n",
    "Avantages : Algorithme d'apprentissage automatique le plus simple et Moins sujet au surajustement et Plus prÃ©cise\n",
    "InconvÃ©nients : Surajustement, ProblÃ¨me avec les relations complexes \n",
    "###### \tRandomForestClassifier\n",
    "Avantages : Parmi toutes les mÃ©thodes de classification disponibles, les forÃªts alÃ©atoires offrent la plus grande prÃ©cision. La technique de la forÃªt alÃ©atoire peut Ã©galement gÃ©rer des donnÃ©es volumineuses avec de nombreuses variables se comptant en milliers. Il peut automatiquement Ã©quilibrer les ensembles de donnÃ©es lorsqu'une classe est moins frÃ©quente que d'autres classes dans les donnÃ©es\n",
    "InconvÃ©nients : La principale limitation de la forÃªt alÃ©atoire est qu'un grand nombre d'arbres peut rendre l'algorithme trop lent et inefficace pour les prÃ©dictions en temps rÃ©el. En gÃ©nÃ©ral, ces algorithmes sont rapides Ã  former, mais assez lents Ã  crÃ©er des prÃ©dictions une fois qu'ils sont formÃ©s.\n",
    "###### \tSVC\n",
    "Avantages : SVM fonctionne relativement bien lorsqu'il existe une marge de sÃ©paration claire entre les classes. SVM est plus efficace dans les espaces de grande dimension et est relativement Ã©conome en mÃ©moire. SVM est efficace dans les cas oÃ¹ les dimensions sont supÃ©rieures au nombre d'Ã©chantillons.\n",
    "InconvÃ©nients : Il ne fonctionne pas bien lorsque nous avons un grand ensemble de donnÃ©es car le temps de formation requis est plus Ã©levÃ©.\n",
    "Il ne fonctionne pas non plus trÃ¨s bien lorsque l'ensemble de donnÃ©es a plus de bruit, c'est-Ã -dire que les classes cibles se chevauchent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e5408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
